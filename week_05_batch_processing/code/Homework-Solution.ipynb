{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024af00e",
   "metadata": {},
   "source": [
    "# Week 5 Homework\n",
    "\n",
    "In this homework, I will put what I've learned about Spark into practice.\n",
    "\n",
    "I'll use the High Volume For-Hire Vehicles (HVFHV) dataset from the New York Taxicab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8280f",
   "metadata": {},
   "source": [
    "## Question 1. Install Spark and PySpark\n",
    "\n",
    "* Install Spark\n",
    "* Run PySpark\n",
    "* Create a local Spark session.\n",
    "* Execute `spark.version`\n",
    "\n",
    "What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3afaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/12 10:06:29 WARN Utils: Your hostname, FONG-DEV-NUC resolves to a loopback address: 127.0.1.1; using 192.168.128.160 instead (on interface eth0)\n",
      "22/03/12 10:06:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/fongt/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/12 10:06:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .appName(\"HW5\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22f6df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0bbdc9",
   "metadata": {},
   "source": [
    "## Question 2. HVFHV February 2021\n",
    "\n",
    "Download the HVFHV data for February 2021:\n",
    "\n",
    "`curl -O https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv`\n",
    "\n",
    "Read it with Spark using the same schema as we did in the lessons.  We'll use this dataset for all the remaining questions.\n",
    "\n",
    "Repartition it to 24 partitions and save it as parquet format.\n",
    "\n",
    "What is the size of the folder (in MB)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be34caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  699M  100  699M    0     0  11.6M      0  0:00:59  0:00:59 --:--:-- 29.6M1:08  0:00:55  0:00:13 14.2M\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee21c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(\"fhvhv_tripdata_2021-02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbfddd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02764|2021-02-01 00:10:40|2021-02-01 00:21:09|          35|          39|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:27:23|2021-02-01 00:44:01|          39|          35|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:28:38|2021-02-01 00:38:27|          39|          91|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:43:37|2021-02-01 01:23:20|          91|         228|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:08:42|2021-02-01 00:17:57|         126|         250|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:26:02|2021-02-01 00:42:51|         208|         243|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:45:50|2021-02-01 01:02:50|         243|         220|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:06:42|2021-02-01 00:31:50|          49|          37|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:34:34|2021-02-01 00:58:13|          37|          76|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:03:43|2021-02-01 00:39:37|          80|         241|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:55:36|2021-02-01 01:08:39|         174|          51|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:06:13|2021-02-01 00:33:45|         235|         129|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:42:24|2021-02-01 01:11:31|         129|         169|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:07:05|2021-02-01 00:20:53|         226|          82|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:28:56|2021-02-01 00:33:59|          82|         129|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:44:53|2021-02-01 01:07:54|           7|          79|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:17:55|2021-02-01 00:34:41|           4|         170|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:38:14|2021-02-01 00:59:20|         164|          42|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:08:04|2021-02-01 00:24:41|         237|           4|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:30:44|2021-02-01 00:41:26|         107|          45|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e06ba10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,StringType,true),StructField(DOLocationID,StringType,true),StructField(SR_Flag,StringType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e565e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a403f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField(\"hvfhs_license_num\" ,types.StringType(), True),\n",
    "    types.StructField(\"dispatching_base_num\" ,types.StringType(), True),\n",
    "    types.StructField(\"pickup_datetime\" ,types.TimestampType(), True),\n",
    "    types.StructField(\"dropoff_datetime\" ,types.TimestampType(), True),\n",
    "    types.StructField(\"PULocationID\" ,types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\" ,types.IntegerType(), True),\n",
    "    types.StructField(\"SR_Flag\" ,types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a26293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(schema) \\\n",
    "        .csv(\"fhvhv_tripdata_2021-02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba41e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02764|2021-02-01 00:10:40|2021-02-01 00:21:09|          35|          39|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:27:23|2021-02-01 00:44:01|          39|          35|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:28:38|2021-02-01 00:38:27|          39|          91|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:43:37|2021-02-01 01:23:20|          91|         228|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:08:42|2021-02-01 00:17:57|         126|         250|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:26:02|2021-02-01 00:42:51|         208|         243|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:45:50|2021-02-01 01:02:50|         243|         220|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:06:42|2021-02-01 00:31:50|          49|          37|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:34:34|2021-02-01 00:58:13|          37|          76|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:03:43|2021-02-01 00:39:37|          80|         241|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:55:36|2021-02-01 01:08:39|         174|          51|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:06:13|2021-02-01 00:33:45|         235|         129|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:42:24|2021-02-01 01:11:31|         129|         169|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:07:05|2021-02-01 00:20:53|         226|          82|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:28:56|2021-02-01 00:33:59|          82|         129|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:44:53|2021-02-01 01:07:54|           7|          79|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:17:55|2021-02-01 00:34:41|           4|         170|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:38:14|2021-02-01 00:59:20|         164|          42|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:08:04|2021-02-01 00:24:41|         237|           4|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:30:44|2021-02-01 00:41:26|         107|          45|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7d8642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "850e149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/12 10:18:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/03/12 10:18:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "22/03/12 10:18:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "22/03/12 10:18:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "22/03/12 10:18:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "22/03/12 10:18:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "22/03/12 10:18:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "22/03/12 10:18:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "22/03/12 10:18:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/03/12 10:18:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/03/12 10:18:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "22/03/12 10:18:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "22/03/12 10:18:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "22/03/12 10:18:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "22/03/12 10:18:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "22/03/12 10:18:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "22/03/12 10:18:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "22/03/12 10:18:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"fhvhv/2021/02/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab026d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 203M\n",
      "-rwxrwxrwx 1 fongt fongt    0 Mar 12 10:18 _SUCCESS\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00000-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00001-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00002-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00003-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00004-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00005-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00006-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00007-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00008-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00009-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00010-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00011-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00012-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00013-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00014-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00015-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00016-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00017-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00018-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00019-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00020-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00021-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00022-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n",
      "-rwxrwxrwx 1 fongt fongt 8.5M Mar 12 10:18 part-00023-f6ba5685-7491-47d8-82d0-209967cbbef3-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh fhvhv/2021/02/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d0cdb",
   "metadata": {},
   "source": [
    "The size of the folder \"fhvhv/2021/02/\" is 203 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad492e",
   "metadata": {},
   "source": [
    "## Question 3. Counting Records\n",
    "\n",
    "How many taxi trips were there on February 15th?\n",
    "\n",
    "Consider only trips that started on February 15th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0530795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"fhvhv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75cd5d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:===========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| Feb15|\n",
      "+------+\n",
      "|367170|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(1) AS Feb15\n",
    "    FROM\n",
    "        fhvhv\n",
    "    WHERE\n",
    "        MONTH(pickup_datetime) = 2 AND DAYOFMONTH(pickup_datetime) = 15;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a71b6c",
   "metadata": {},
   "source": [
    "There were 367,170 taxi trips that started on February 15, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60ec13e",
   "metadata": {},
   "source": [
    "## Question 4. Longest Trip for Each Day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "\n",
    "Trip starting on which day was the longest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c82956c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:============================================>           (19 + 5) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|pickup_date|sec_diff|\n",
      "+-----------+--------+\n",
      "| 2021-02-11|   75540|\n",
      "| 2021-02-17|   57221|\n",
      "| 2021-02-20|   44039|\n",
      "| 2021-02-03|   40653|\n",
      "| 2021-02-19|   37577|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        TO_DATE(pickup_datetime) AS pickup_date,\n",
    "        UNIX_TIMESTAMP(dropoff_datetime) - UNIX_TIMESTAMP(pickup_datetime) AS sec_diff\n",
    "    FROM\n",
    "        fhvhv\n",
    "    ORDER BY \n",
    "        sec_diff DESC\n",
    "    LIMIT\n",
    "        5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b538c",
   "metadata": {},
   "source": [
    "It appears the longest trip occurred on February 11, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ab75d",
   "metadata": {},
   "source": [
    "## Question 5. Most Frequent `dispatching_base_num`\n",
    "\n",
    "Now find the most frequently occuring `dispatching_base_num` in this dataset.\n",
    "\n",
    "How many stages does this Spark job have?\n",
    "\n",
    "> **_NOTE:_** The answer may depend on how you write the query, so there are multiple correct answers.  Select the one you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab416090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===========================>                           (12 + 12) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|num_disp_base_num|dispatching_base_num|\n",
      "+-----------------+--------------------+\n",
      "|          3233664|              B02510|\n",
      "|           965568|              B02764|\n",
      "|           882689|              B02872|\n",
      "|           685390|              B02875|\n",
      "|           559768|              B02765|\n",
      "+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(1) AS num_disp_base_num,\n",
    "        dispatching_base_num\n",
    "    FROM \n",
    "        fhvhv\n",
    "    GROUP BY\n",
    "        dispatching_base_num\n",
    "    ORDER BY\n",
    "        num_disp_base_num DESC\n",
    "    LIMIT\n",
    "        5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a2992f",
   "metadata": {},
   "source": [
    "The most common `dispatching_base_num` is \"B02510\".\n",
    "\n",
    "The Spark job had 4 stages:\n",
    "\n",
    "![Question 5 Spark Job Pic](./images/hw-q5-jobs.png \"Question 5 Spark Job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f364b0",
   "metadata": {},
   "source": [
    "## Question 6. Most Common Locations Pair\n",
    "\n",
    "Find the most common pickup-dropoff pair.\n",
    "\n",
    "For example:\n",
    "\"Jamaica Bay / Clinton East\"\n",
    "\n",
    "Enter two zone names separated by a slash.\n",
    "\n",
    "If any of the zone names are unknown (missing), use \"Unknown.\"  For example, \"Unknown / Clinton East.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "122fe462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 12322  100 12322    0     0    797      0  0:00:15  0:00:15 --:--:--  2968\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d10c55e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .csv(\"taxi+_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2c8e0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3b35ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(LocationID,StringType,true),StructField(Borough,StringType,true),StructField(Zone,StringType,true),StructField(service_zone,StringType,true)))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "615ad93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_schema = types.StructType([\n",
    "    types.StructField(\"LocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"Borough\", types.StringType(), True),\n",
    "    types.StructField(\"Zone\", types.StringType(), True),\n",
    "    types.StructField(\"service_zone\", types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "546ffe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .schema(zone_schema) \\\n",
    "            .csv(\"taxi+_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac232adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44531081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.registerTempTable(\"zones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "502821ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=====================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+----------+\n",
      "|PUDOpair                               |num_occurs|\n",
      "+---------------------------------------+----------+\n",
      "|East New York/East New York            |45041     |\n",
      "|Borough Park/Borough Park              |37329     |\n",
      "|Canarsie/Canarsie                      |28026     |\n",
      "|Crown Heights North/Crown Heights North|25976     |\n",
      "|Bay Ridge/Bay Ridge                    |17934     |\n",
      "+---------------------------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CONCAT(COALESCE(puzones.zone, 'Unknown'), '/', COALESCE(dozones.zone, 'Unknown')) AS PUDOpair,\n",
    "        COUNT(1) AS num_occurs\n",
    "    FROM\n",
    "        fhvhv\n",
    "            LEFT JOIN zones AS puzones\n",
    "                ON fhvhv.PULocationID = puzones.LocationID\n",
    "            LEFT JOIN zones AS dozones\n",
    "                ON fhvhv.DOLocationID = dozones.LocationID\n",
    "    GROUP BY\n",
    "        PUDOpair\n",
    "    ORDER BY\n",
    "        num_occurs DESC\n",
    "    LIMIT\n",
    "        5;\n",
    "\"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c80ea1",
   "metadata": {},
   "source": [
    "The most common locations pair is East New York/East New York."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2ec3b",
   "metadata": {},
   "source": [
    "## Bonus Question.  Join Type\n",
    "\n",
    "For finding the answer to question 6, you'll need to perform a join.\n",
    "\n",
    "What type of join is it?\n",
    "\n",
    "How many stages does your Spark job have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd4dee",
   "metadata": {},
   "source": [
    "The type of join is a broadcast join:\n",
    "\n",
    "![Question 6 Join Type](./images/hw-bonus-join-type.png \"Question 6 Join Type\")\n",
    "\n",
    "My Spark job has 4 stages:\n",
    "\n",
    "![Question 6 Spark Job Pic](./images/hw-q6-jobs.png \"Question 6 Spark Job\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
